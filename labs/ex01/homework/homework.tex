\documentclass[twoside,11pt]{article}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[english]{babel}	
\usepackage{paralist}	
\usepackage[lowtilde]{url}
\usepackage{fixltx2e}
\usepackage{listings}
\usepackage{color}
\usepackage{hyperref}
\usepackage{amssymb}


\renewcommand{\thesubfigure}{\roman{subfigure}}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}


\begin{document}
	
\title{Week 1}
\date{\vspace{-5ex}}
\maketitle

\section{Convexity}

\subsection{Exercise 1}

We will prove Jensen's inequality using induction.
For $m=2$ it holds from the definition of \emph{convex function}:
\begin{equation*}
f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda) f(x_2)
\end{equation*}
Now, if for m=k is true we will prove that for m=k+1 is also true:
\begin{align}
	f\left(\sum_{i=1}^{k+1} \lambda_{i} x_{i}\right) & = f\left(\sum_{i=1}^{k} \lambda_{i} x_{i} + \lambda_{k+1}x_{k+1}\right) \\
	& = f\left(\left(\sum_{j=1}^{k} \lambda_j \right) \left(\sum_{i=1}^{k} \frac{\lambda_{i}}{\left(\sum_{j=1}^{k} \lambda_j \right)} x_{i} \right) + \lambda_{k+1}x_{k+1}\right) \\
	& \leq \left(\sum_{j=1}^{k} \lambda_j \right) f\left(\sum_{i=1}^{k} \frac{\lambda_{i}}{\left(\sum_{j=1}^{k} \lambda_j \right)} x_{i} \right) + \lambda_{k+1}f(x_{k+1}) \label{use_cf_def_1} \\ 
	& \leq \sum_{i=1}^{k} \lambda_{i} f(x_{i}) + \lambda_{k+1}f(x_{k+1}) \label{use_induction_1} \\ 
	& = \sum_{i=1}^{k+1} \lambda_{i} f(x_{i})
\end{align}
where in \eqref{use_cf_def_1}, we have used the fact that f is a convex function and in \eqref{use_induction_1}, that Jensen's inequality holds for m=k.

\subsection{Exercise 2}

Since f is a convex function, we can apply the convexity definition on the cube boundary points. So, any point between each pair of corner points must have lower value. That includes the edges. We can pick any two points from the edges, and apply the convexity property to all points inside the cube. From that, it is trivial that the maximum value must be in one of the corners.

Since the values of the function are bounded inside the cube, there cannot be any discontinuity. Say that in $f(x_d)$ there is a gap of size $K$ from negative to positive values, then between $f(x_d - 2\epsilon)$ and $f(x_d + 2\epsilon)$ it cannot be convex, because $f(x_d + \epsilon) > 0.25 f(x_d - 2\epsilon) + 0.75 f(x_d + 2\epsilon) \approx f(x_d + \epsilon) - 0.25K$.

\subsection{Exercise 3}

$||x - y||^{2}$ is twice differentiable. The Hessian matrix is $2\mathbb{I}$, which is positive definite. So the function is convex in all the domain.


\subsection{Exercise 4}


(i) 
\begin{align*}
	g(\beta x + (1 -\beta) y) & = \sum_{i} \lambda_{i}f_{i}(\beta x + (1 -\beta) y) \\ 
	& \leq \sum_{i} \lambda_{i}(\beta f_{i}(x) + (1 - \beta) f_{i}(y)) \\
	& = \beta \sum_{i} \lambda_{i} f_{i}(x) + (1 - \beta) \sum_{i} \lambda_{i} f_{i} (y)
\end{align*}

(ii) Since g(x) maps to the domain of f for all x, and f is convex, f(g(x)) is convex.

\subsection{Exercise 7}

\begin{align*}
	\sum_{i} |\lambda_i x_i + (1 - \lambda_i) y_i| \leq \sum_{i} \lambda_i |x_i| + (1 - \lambda_i) |y_i|
\end{align*}

\subsection{Exercise 8}

\begin{align*}
	f(\lambda x + (1 - \lambda) y) & \leq f(\lambda x) + f((1 - \lambda) y) \\ & = \lambda f(x) + (1 - \lambda) f(y)
\end{align*}
the equality holds because $\lambda \in [0, 1]$

\section{Gradient Descent}

\subsection{Exercise 5}

\subsection{Exercise 6}

\subsection{Exercise 9}

\subsection{Exercise 11}
	
\end{document}